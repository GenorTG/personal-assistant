# LLM Service Requirements
# Install with: pip install -r requirements.txt
# 
# For CUDA support, first set environment variable:
#   Windows PowerShell: $env:CMAKE_ARGS="-DGGML_CUDA=on"
#   Linux/Mac: CMAKE_ARGS="-DGGML_CUDA=on"
# Then: pip install -r requirements.txt --no-cache-dir

# Core LLM library - will be built with CUDA if CMAKE_ARGS is set
llama-cpp-python[server]>=0.3.0

# FastAPI server dependencies
fastapi>=0.100.0
uvicorn[standard]>=0.23.0
pydantic>=2.0.0

# HTTP client for health checks
httpx>=0.24.0
aiohttp>=3.8.0

# Utilities
python-dotenv>=1.0.0
